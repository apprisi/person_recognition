{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import cv2\n",
    "\n",
    "# caffe layers\n",
    "caffe_root = '/users/vijay.kumar/caffe/'\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "sys.path.insert(0, '/users/vijay.kumar/tools/liblinear-2.1/python')\n",
    "\n",
    "import caffe\n",
    "from caffe import layers as L\n",
    "from liblinearutil import *\n",
    "from utils import *\n",
    "\n",
    "# enable gpu\n",
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = read_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initilaze caffe transformer\n",
    "transformer = define_transformer()\n",
    "\n",
    "# load pose nets and the pose estimator\n",
    "nets, pose_net = load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "from cStringIO import StringIO\n",
    "import IPython.display\n",
    "\n",
    "def showarray(a, fmt='png'):\n",
    "    a = np.uint8(a)\n",
    "    f = StringIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    IPython.display.display(IPython.display.Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imdb labeled training set.\n",
    "imdb_imgs_dir = '../data/imdb/images/'\n",
    "imdb_annot_dir = '../data/imdb/annot/'\n",
    "\n",
    "imdb_annot_files = os.listdir(imdb_annot_dir)\n",
    "labeled_actors = [int(_f.split('.')[0]) for _f in imdb_annot_files] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_models = len(nets)\n",
    "num_examples = 0\n",
    "# get count of number of examples for initialization.\n",
    "for _f in imdb_annot_files:\n",
    "    annotations = np.genfromtxt(imdb_annot_dir + _f, dtype = float, delimiter=',')\n",
    "    if len(annotations.shape)==1:\n",
    "        annotations = annotations.reshape((1,5))    \n",
    "    num_examples = num_examples + len(annotations)\n",
    "    \n",
    "# initiliaze train set\n",
    "train_features = np.zeros((num_examples, num_models, params['FEATSIZE']))\n",
    "train_labels = np.zeros((num_examples,1))   \n",
    "\n",
    "# extract features\n",
    "count = 0\n",
    "\n",
    "for la in labeled_actors:\n",
    "    print 'Extracting features for subject:', la\n",
    "    annotations = loadtxt(imdb_annot_dir + str(la) + '.txt', dtype = float, delimiter=',')\n",
    "    \n",
    "    if len(annotations.shape) == 1:\n",
    "        annotations = annotations.reshape((1,5))\n",
    "        \n",
    "    for annot_sample in annotations:        \n",
    "        img_name = imdb_imgs_dir + str(la) + '/' + str(int(annot_sample[0])) + '.jpg'        \n",
    "        image = cv2.imread(img_name)     \n",
    "        hbox = annot_sample[1:] \n",
    "\n",
    "        head = get_region_imdb(image, hbox, 'HEAD')\n",
    "        upper_body = get_region_imdb(image, hbox, 'UB')       \n",
    "                                                           \n",
    "        # get psm features\n",
    "        train_features[count] = get_pose_features(transformer, nets, head, upper_body, num_models, params['FEATSIZE'])\n",
    "        train_labels[count] = la            \n",
    "        \n",
    "        count = count + 1  \n",
    "\n",
    "train_features = train_features[range(count)]\n",
    "train_labels = train_labels[range(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Classifier training\n",
    "classifiers = train_linear_classifiers(train_features, np.squeeze(train_labels), num_models, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "faces = loadtxt('../data/hannah/annotations/hannah_video_faces_.txt')\n",
    "track_char_map = loadtxt('../data/hannah/annotations/hannah_video_tracks.txt', usecols=(0,1), skiprows=2)\n",
    "track_char_dict_map = {}\n",
    "for i in range(track_char_map.shape[0]):\n",
    "    track_char_dict_map[track_char_map[i,0]] = track_char_map[i,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# movie is split into four small 1GB videos..\n",
    "video_names = ['../data/hannah/videos/hannah-001_1_37784.vob', '../data/hannah/videos/hannah-002_37785_75444.vob', \n",
    "             '../data/hannah/videos/hannah-003_75445_115473.vob','../data/hannah/videos/hannah-004_115474_153475.vob']\n",
    "\n",
    "video_frame_split = np.zeros((5,))\n",
    "for i in range(4):\n",
    "    cap = cv2.VideoCapture(video_names[i])\n",
    "    video_frame_split[i+1] = video_frame_split[i] + cap.get(cv2.cv.CV_CAP_PROP_FRAME_COUNT)\n",
    "    cap.release()\n",
    "print video_frame_split\n",
    "print \"total_frames:\", np.sum(video_frame_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_frame_no(fno, vf_split, vid_objs):\n",
    "    for i in range(4):\n",
    "        if fno > vf_split[i] and fno <= vf_split[i+1]:\n",
    "            fno = fno - vf_split[i]\n",
    "            working_cap = vid_objs[i]\n",
    "    return working_cap, fno\n",
    "\n",
    "video_objs = {} \n",
    "for i in range(4):\n",
    "    video_objs[i] = cv2.VideoCapture(video_names[i])    \n",
    "    success,image = video_objs[i].read()    \n",
    "    \n",
    "frame_interval = 1\n",
    "track_ids = faces[:,5]\n",
    "unique_track_ids = np.unique(track_ids)\n",
    "num_tracks = len(unique_track_ids)\n",
    "\n",
    "test_labels = []\n",
    "test_tracks = []\n",
    "pred_scores = []\n",
    "for i in range(num_tracks):       \n",
    "    \n",
    "    tid = unique_track_ids[i]\n",
    "    print 'Processing track:',tid\n",
    "    \n",
    "    # obtain track, frame and facebb details.\n",
    "    track_data = faces[np.where(track_ids==tid)]\n",
    "    track_data = track_data[0::frame_interval,:]\n",
    "    frame_nos = track_data[:,0]                    \n",
    "    num_frames = len(frame_nos)\n",
    "    \n",
    "    for j in range(num_frames):                             \n",
    "        \n",
    "        # get corresponding frame from movie\n",
    "        cap, fno = get_frame_no(frame_nos[j], video_frame_split, video_objs)\n",
    "                \n",
    "        # there is a misplacement of 3 frames between the movie video I used and annotation provided.. \n",
    "        # adjusting it.\n",
    "        cap.set(cv2.cv.CV_CAP_PROP_POS_FRAMES, fno-3) \n",
    "        \n",
    "        # read frame\n",
    "        fbox = track_data[j,1:5]             \n",
    "        success, image = cap.read()          \n",
    "                \n",
    "        if success:\n",
    "            # Resize the frame as the annotations provided by Ozerov et al. are with respect to 996x560 frame size. \n",
    "            image = cv2.resize(image,(996, 560))                   \n",
    "            \n",
    "            # crop head and ub\n",
    "            head = get_region_hannah(image, fbox, 'HEAD')\n",
    "            upper_body = get_region_hannah(image, fbox, 'UB')\n",
    "           \n",
    "            # not considering regions with <10 pixels.\n",
    "            if head.shape[0] < 10 or head.shape[1] < 10 or upper_body.shape[0] < 10 or upper_body.shape[1] < 10:             \n",
    "                continue                                                                       \n",
    "\n",
    "            # obtain psm feature\n",
    "            test_feature = get_pose_features(transformer, nets, head, upper_body, num_models, params['FEATSIZE'])            \n",
    "            \n",
    "            # get pose weights\n",
    "            pose_weights = get_pose_weights(transformer, pose_net, upper_body)                                        \n",
    "                            \n",
    "            # identify label            \n",
    "            pred_sample_sc = pose_aware_identity_prediction_(classifiers, test_feature, np.array([track_char_dict_map[tid]]), pose_weights,  params, num_models)        \n",
    "            \n",
    "            # store data\n",
    "            test_labels.append(int(track_char_dict_map[tid]))\n",
    "            test_tracks.append(tid)\n",
    "            pred_scores.append(pred_sample_sc[0]) \n",
    "        \n",
    "for i in range(4):            \n",
    "    video_objs[i].release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain predicted labels\n",
    "model_labels = classifiers[0][0].get_labels()\n",
    "pred_labels = [model_labels[np.argmax(ps)] for ps in pred_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Overall accuracy including known and unknown subjects\n",
    "overall_acc = 100*np.mean(np.squeeze(np.array(test_labels)) == np.squeeze(np.array(pred_labels)))\n",
    "print 'overall accuracy:', overall_acc\n",
    "print\n",
    "\n",
    "# Overall accuracy of known subjects\n",
    "correct = 0\n",
    "num_examples_labeled_actors = 0\n",
    "correct_per_labeled_actor = {}\n",
    "num_examples_per_labeled_actor = {}\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] in labeled_actors:\n",
    "        num_examples_labeled_actors = num_examples_labeled_actors + 1\n",
    "        \n",
    "        # initiliaze dict\n",
    "        if test_labels[i] not in num_examples_per_labeled_actor:\n",
    "            num_examples_per_labeled_actor[test_labels[i]] = 0  \n",
    "            correct_per_labeled_actor[test_labels[i]] = 0\n",
    "        \n",
    "        num_examples_per_labeled_actor[test_labels[i]] = num_examples_per_labeled_actor[test_labels[i]] + 1\n",
    "            \n",
    "        if test_labels[i]==pred_labels[i]:\n",
    "            correct = correct + 1            \n",
    "            correct_per_labeled_actor[test_labels[i]] = correct_per_labeled_actor[test_labels[i]] + 1\n",
    "\n",
    "labeled_accuracy = 100*correct/float(num_examples_labeled_actors)            \n",
    "print 'Overall accuracy of Known subjects:'\n",
    "print '     #correct:', correct, '#samples', num_examples_labeled_actors, 'Accuracy:', labeled_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
